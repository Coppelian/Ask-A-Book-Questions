{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to ask a book a question?\n",
        "\n",
        "## Who is this for?\n",
        "\n",
        "Students who know basic Python and want AI to answer questions about long documents (books, PDFs, notes).\n",
        "\n",
        "You’ll learn the core RAG pipeline and build intuition for each step.\n",
        "\n",
        "## Learning outcomes\n",
        "\n",
        "Explain what RAG is and why we use it\n",
        "\n",
        "Load + chunk a PDF/book, embed the chunks, store & search them, then ask targeted questions\n",
        "\n",
        "Avoid common pitfalls (bad chunking, poor retrieval, prompt mistakes)\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "RAG = Retrieval + Generation\n",
        "\n",
        "Retrieve: pull the most relevant snippets (\"chunks\") of your book from a vector database using semantic similarity.\n",
        "\n",
        "Augment: give those snippets to an LLM as context.\n",
        "\n",
        "Generate: the LLM writes an answer grounded in the retrieved text.\n",
        "\n",
        "### Why not just ask the LLM?\n",
        "\n",
        "LLMs may hallucinate and can’t “remember” entire books.\n",
        "\n",
        "RAG anchors answers in your exact sources (cite or include snippets).\n",
        "\n",
        "## What will we learn here?\n",
        "\n",
        "In this code demonstration, we're going to guide you through the following targets to understand how Retrieval Augmented Generation(RAG) works:\n",
        "\n",
        "\n",
        "1.   Load the document (PDF/HTML/Markdown)\n",
        "\n",
        "2. Split into overlapping chunks\n",
        "\n",
        "3. Create embeddings (vector representation) of chunks\n",
        "\n",
        "4. Store vectors in ChromaDB (local) or Pinecone (cloud)\n",
        "\n",
        "5. Query: embed your question → find top‑k similar chunks\n",
        "\n",
        "6. Prompt the LLM with those chunks + your question\n",
        "\n",
        "7. Answer with citations/snippets\n",
        "\n",
        "![Fig.1 How does RAG works.](https://drive.google.com/uc?export=view&id=11xecWxSs5qiUQQHeY3lpkST0YLaOrjSw)\n",
        "\n",
        "Fig.1 How does RAG works.\n"
      ],
      "metadata": {
        "id": "h6rY7upEQ2gg"
      },
      "id": "h6rY7upEQ2gg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install/Upgrade Dependencies (Modern Stack)\n",
        "\n",
        "- Installs/updates packages\n",
        "- Removes RAPIDS (optional) in Google Colab to avoid pyarrow pinning\n",
        "- Installs the core stack (OpenAI/LangChain/Chroma/Pinecone etc.)\n",
        "\n",
        "Refs:\n",
        "- pip user guide: https://pip.pypa.io/\n",
        "- LangChain split pkgs: https://python.langchain.com/"
      ],
      "metadata": {
        "id": "atIWGj-kOeZ-"
      },
      "id": "atIWGj-kOeZ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d615a77",
      "metadata": {
        "id": "9d615a77"
      },
      "outputs": [],
      "source": [
        "# Update manual construction tool (latest-oriented setup)\n",
        "# 1) Optional: remove RAPIDS if present to avoid pyarrow constraints\n",
        "# !pip uninstall -y cudf-cu12 pylibcudf-cu12 || true\n",
        "\n",
        "# 2) Upgrade build tools\n",
        "!pip install -qU pip setuptools wheel\n",
        "\n",
        "# 3) Base pins for this environment (latest compatible)\n",
        "#    - requests: you asked for 2.32.5 (note: Google Colab sometimes pins 2.32.4)\n",
        "!pip install -qU \"requests==2.32.5\" \"jedi>=0.16\"\n",
        "\n",
        "# 4) Install core stack (no old pinecone-client)\n",
        "#    - Use pyarrow>=21 to satisfy modern datasets\n",
        "!pip install -qU \\\n",
        "  \"pyarrow>=21\" \\\n",
        "  \"transformers>=4.45,<5\" \\\n",
        "  \"tokenizers>=0.20.1,<0.21\" \\\n",
        "  \"sentence-transformers>=3,<4\" \\\n",
        "  \"langchain>=0.3.27,<0.4\" \\\n",
        "  \"langchain-community>=0.3.7\" \\\n",
        "  \"langchain-openai>=0.2.12\" \\\n",
        "  \"datasets>=4.1.1\" \\\n",
        "  \"accelerate>=0.33\" \\\n",
        "  einops \\\n",
        "  pypdf \\\n",
        "  pdfminer.six \\\n",
        "  pinecone \\\n",
        "  chromadb\n",
        "\n",
        "# (Optional GPU extras)\n",
        "# pip install -qU xformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. PDF loader options (unstructured/PyPDF) and text splitting\n",
        "Shows two strategies to load PDFs (Unstructured vs PyPDF) and a text splitter configuration.\n",
        "\n",
        "\n",
        "*   **Unstructured loader** can parse complex PDFs but sometimes needs optional system deps.\n",
        "\n",
        "*   **PyPDFLoader** is simpler/reliable for most text PDFs.\n",
        "\n",
        "*   A splitter (e.g., **CharacterTextSplitter**) controls chunk size/overlap for embeddings.\n",
        "\n",
        "Please check Langchain Document for more informations: https://python.langchain.com/docs/integrations/document_loaders/\n",
        "\n"
      ],
      "metadata": {
        "id": "zoru1xJuSLF3"
      },
      "id": "zoru1xJuSLF3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3e92ed",
      "metadata": {
        "id": "2d3e92ed"
      },
      "outputs": [],
      "source": [
        "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a2d6bf",
      "metadata": {
        "id": "b4a2d6bf"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"Flink in Action.pdf\")\n",
        "# loader = PyPDFLoader(\"field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")\n",
        "## Other options for loaders\n",
        "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdac23c",
      "metadata": {
        "id": "bcdac23c"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fd7c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fd7c9e",
        "outputId": "31ef27d8-7140-47aa-cec5-3677f695b61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 35 document(s) in your data\n",
            "There are 2388 characters in your document\n"
          ]
        }
      ],
      "source": [
        "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[30].page_content)} characters in your document')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) How to load everything into data variable?"
      ],
      "metadata": {
        "id": "lUBw7AKyYeIu"
      },
      "id": "lUBw7AKyYeIu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use new document loader\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, OnlinePDFLoader, UnstructuredPDFLoader\n",
        ")\n",
        "\n",
        "loaders = [\n",
        "    PyPDFLoader(\"Flink in Action.pdf\"),\n",
        "    PyPDFLoader(\"field-guide-to-data-science.pdf\"),\n",
        "    OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\"),\n",
        "    UnstructuredPDFLoader(\"Learning_Apache_Flink.pdf\"),\n",
        "    # add more files/URL...\n",
        "]\n",
        "\n",
        "data = []\n",
        "for ld in loaders:\n",
        "    try:\n",
        "        docs = ld.load()          # every loader returns List[Document]\n",
        "        data.extend(docs)         # add to data\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] skip {ld.__class__.__name__}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(data)} Document chunks from {len(loaders)} loaders.\")\n",
        "# Optional: see the source\n",
        "sources = {d.metadata.get('source') for d in data}\n",
        "print(\"Unique sources:\", len(sources))"
      ],
      "metadata": {
        "id": "TBdTP-cOYkUF"
      },
      "id": "TBdTP-cOYkUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8af9b604",
      "metadata": {
        "id": "8af9b604"
      },
      "source": [
        "## 3. Chunk your data up into smaller documents\n",
        "* **How it works:** Builds/uses a TextSplitter to convert long page texts into smaller chunks for better embedding/search.\n",
        "\n",
        "* **Why:** Applies split_documents(docs) to get many shorter Document chunks optimized for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3c6f02",
      "metadata": {
        "id": "fb3c6f02"
      },
      "outputs": [],
      "source": [
        "# Note: If you're using PyPDFLoader then we'll be splitting for the 2nd time.\n",
        "# This is optional, test out on your own data.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check an example\n",
        "print(texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8uqAi9AMnF7",
        "outputId": "05c6cf95-6d76-4965-b4a0-ffe63999b9b5"
      },
      "id": "O8uqAi9AMnF7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='welcome \n",
            "Thank you for purchasing the MEAP for Flink in Action . We are excited to deliver this book as \n",
            "large-scale stream processing using Flink and Google Data Flow is fast gaining in popularity. \n",
            "Stream processing is much more than just processing records one at a time as they arrive. \n",
            "True stream processing needs support for concepts such as event time processing to ensure \n",
            "stream processing systems are just as accurate as the batch processing system. There is a \n",
            "need for one system the performs both stream a nd batch processing. Apache Flink is that \n",
            "system.  \n",
            "As we started exploring Apache Flink, we discovered the subtle challenges that are \n",
            "inherent in stream processing. These challenges are intrinsic to how stream processing is \n",
            "performed. Unlike batch processing, where all data is available when processing begins, \n",
            "stream processing must be able to handle incomplete data, late arrivals, and out -of-order \n",
            "arrivals—without compromising performance or accuracy —and be resilient to failure. We \n",
            "tackle all these challenges in this book.  \n",
            "Writing this book has been a challenge, partly because the technology is changing rapidly \n",
            "as we write and partly because we want to make this complex topic of streaming easy to \n",
            "understand in the context of everyday use cases. We believe that eventually streaming \n",
            "systems will become the norm, because the real world operates in the streaming mode. Real -\n",
            "world events occur and are captured continuously in transaction systems. The reporting \n",
            "systems that aggregate these transactions into reports operate in batch -processing mode due \n",
            "to technology limitations. These limitations are now being addressed by systems such as \n",
            "Apache Flink. We hope this book helps you develop a strong foundation in the concepts and \n",
            "the challenges of implementing streamin g systems capable of handling high -velocity and high-\n",
            "volume streaming data. \n",
            "Please be sure to post any questions, comments, or suggestions you have about the book' metadata={'producer': 'Adobe Acrobat Pro 10.0.0', 'creator': 'Adobe Acrobat Pro 10.0.0', 'creationdate': '2016-09-28T03:24:53+02:00', 'author': 'Sameer B. Wadkar and Hari Rajaram', 'moddate': '2016-09-28T03:29:37+02:00', 'subject': 'MEAP Version 2', 'title': 'Flink in Action MEAP V02', 'source': 'Flink in Action.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879873a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879873a4",
        "outputId": "bf7c9aaf-eea1-436f-89b7-8f958be2b032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 51 documents\n"
          ]
        }
      ],
      "source": [
        "# Check the contents\n",
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b2843",
      "metadata": {
        "id": "838b2843"
      },
      "source": [
        "### 4. Create embeddings (OpenAI): Create embeddings of your documents to get ready for semantic search\n",
        "*   **Purpose:** Initializes OpenAIEmbeddings for vectorization\n",
        "\n",
        "*   **How does it works:** Uses your OPENAI_API_KEY and an embedding model (often text-embedding-3-small, 1536-d) to support similarity.\n",
        "\n",
        "*   **Notice:** Please make sure you have a .env file at the workspace containing OPENAI_API_KEY."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Environment Vars & Embeddings"
      ],
      "metadata": {
        "id": "A_G76aAPT2su"
      },
      "id": "A_G76aAPT2su"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373e695a",
      "metadata": {
        "id": "373e695a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a71a25c-0396-4b95-ab94-9cc3412114f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma-only setup: embeddings ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 10\n",
        "import os\n",
        "\n",
        "# load OPENAI_API_KEY\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "assert OPENAI_API_KEY, \"Missing OPENAI_API_KEY. Put it in environment or a .env file.\"\n",
        "\n",
        "# Use OpenAI Embedding\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Chroma-only setup: embeddings ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Init ChromaDB\n",
        "What we did is the following steps. Check Document to see how ChromaDB works: https://docs.trychroma.com/docs/overview/introduction\n",
        "\n",
        "\n",
        "1.   Establish local connections for ChromaDB\n",
        "2.   Convert text to vectors and insert them into local ChromaDB\n",
        "3.   Set up retriever.\n",
        "\n"
      ],
      "metadata": {
        "id": "sHneTBh_UBgF"
      },
      "id": "sHneTBh_UBgF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e093ef3",
      "metadata": {
        "hide_input": false,
        "id": "0e093ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc42c127-a410-4eff-acca-24684453daf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-277809409.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  chroma = Chroma(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma collection ready: llama-2-rag (persist at ./chroma_store)\n"
          ]
        }
      ],
      "source": [
        "CHROMA_DIR = \"./chroma_store\"     # Local Dir\n",
        "collection_name = \"llama-2-rag\"   # Name for the instance\n",
        "\n",
        "# load database\n",
        "chroma = Chroma(\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=CHROMA_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Chroma collection ready: {collection_name} (persist at {CHROMA_DIR})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) How to create a ChromaDB and interact with it."
      ],
      "metadata": {
        "id": "I6mBgdXrXGvG"
      },
      "id": "I6mBgdXrXGvG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert datas into Chroma using Embedding\n",
        "from typing import Iterable\n",
        "\n",
        "def _to_strings(items: Iterable):\n",
        "    out = []\n",
        "    for t in items:\n",
        "        if hasattr(t, \"page_content\"):\n",
        "            out.append(t.page_content)\n",
        "        else:\n",
        "            out.append(str(t))\n",
        "    return out\n",
        "\n",
        "# Suppose we have `texts`, which we have defined above\n",
        "assert \"texts\" in globals(), \"Variable `texts` not found. Please define it before running this cell.\"\n",
        "\n",
        "text_list = _to_strings(texts)\n",
        "# If you need to manually define ids / metadatas，you can pass them in this API\n",
        "chroma.add_texts(texts=text_list)      # metadatas=..., ids=...\n",
        "chroma.persist()\n",
        "\n",
        "print(f\"Upserted {len(text_list)} texts into Chroma and persisted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5c_eaTX3KOL",
        "outputId": "b0407146-109b-46be-ff80-9db078f28008"
      },
      "id": "L5c_eaTX3KOL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 51 texts into Chroma and persisted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0d1c6a",
      "metadata": {
        "id": "4e0d1c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ac689d-76cf-4b4a-9514-46f18be54a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever ready (Chroma).\n"
          ]
        }
      ],
      "source": [
        "retriever = chroma.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"Retriever ready (Chroma).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. (Danger/Optional) Clear Data Helper\n",
        "\n",
        "Clears all vectors in the given namespace (default: `__default__`)."
      ],
      "metadata": {
        "id": "LzPGLu5-SAj2"
      },
      "id": "LzPGLu5-SAj2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15\n",
        "def chroma_delete_by_ids(id_list):\n",
        "    \"\"\"Delete using ID\"\"\"\n",
        "    chroma.delete(ids=id_list)\n",
        "    chroma.persist()\n",
        "    print(f\"Deleted {len(id_list)} ids from Chroma.\")\n",
        "\n",
        "def chroma_delete_where(where: dict):\n",
        "    \"\"\"Delete Using Metadata\"\"\"\n",
        "    _ = chroma._collection.delete(where=where)\n",
        "    chroma.persist()\n",
        "    print(f\"Deleted by condition: {where}\")\n",
        "\n",
        "def chroma_clear_all():\n",
        "    \"\"\"Delete all\"\"\"\n",
        "    _ = chroma._collection.delete(where={})\n",
        "    chroma.persist()\n",
        "    print(f\"Cleared all data in Chroma collection '{collection_name}'.\")\n",
        "\n",
        "\n",
        "# chroma_delete_by_ids([\"doc-1\", \"doc-2\"])\n",
        "# chroma_delete_where({\"source\": {\"$eq\": \"note\"}})\n",
        "# chroma_clear_all()\n"
      ],
      "metadata": {
        "id": "vcA4F5BBR9z-"
      },
      "id": "vcA4F5BBR9z-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Interact with ChromaDB"
      ],
      "metadata": {
        "id": "_fEz2WVnX7Td"
      },
      "id": "_fEz2WVnX7Td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388988ce",
      "metadata": {
        "id": "388988ce"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Use ChromaDB to construct Vector Database（Locally，Can change collection name and directory）\n",
        "docsearch = Chroma.from_texts(\n",
        "    texts=[t.page_content for t in texts],\n",
        "    embedding=embeddings,\n",
        "    collection_name=collection_name,\n",
        "    persist_directory=\"./chroma_store\",  # 本地存储路径\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34929595",
      "metadata": {
        "id": "34929595"
      },
      "outputs": [],
      "source": [
        "query = \"What is flink?\"\n",
        "docs = docsearch.similarity_search(query, k=5)\n",
        "# Search similarity based on cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0f5b45",
      "metadata": {
        "id": "4e0f5b45",
        "outputId": "78415392-d84b-4a92-9e99-61325a3079f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEAP Edition \n",
            "Manning Early Access Program \n",
            "Flink in Action \n",
            "Version 2 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Copyright 2016 Manning Publications \n",
            " \n",
            " \n",
            "For more information on this and other Manning titles go to  \n",
            "www.manning.com \n",
            "©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \n",
            "other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \n",
            "https://forums.manning.com/forums/flink-in-action\n"
          ]
        }
      ],
      "source": [
        "# Here's an example of the first document that was returned\n",
        "print(docs[3].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQb2UCn8M-p4",
        "outputId": "380b3aaf-7dc9-48e6-ae27-4057e9d15698"
      },
      "id": "SQb2UCn8M-p4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content='https://forums.manning.com/forums/flink-in-action\\n4'), Document(metadata={}, page_content='brief contents \\nPART 1:  STREAM PROCESSING USING FLINK \\n  1  Introducing Apache Flink \\n  2  Getting started with Flink  \\n  3  Batch processing using the DataSet API \\n  4  Stream processing using the DataStream API \\n  5  Basics of event time processing  \\nPART 2:  ADVANCED STREAM PROCESSING USING FLINK \\n  6  Session windows and custom windows \\n  7  Using the Flink API in practice  \\n  8  Using Kafka with Flink \\n  9  Fault tolerance in Flink   \\nPART 3:  OUT IN THE WILD  \\n10  Domain-specific libraries in Flink – CEP and Streaming SQL \\n11  Apache Beam and Flink \\nAPPENDIXES: \\nA   Setting up your local Flink environment \\nB   Installing Apache Kafka \\n \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action'), Document(metadata={}, page_content='streaming provides Flink with a more fine -grained ability to process data. \\nIn the next chapter we will show you how to install Flink and write simple programs in Flink \\nusing the DataSet, DataStream and Table APIs of Flink.  \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action\\n31'), Document(metadata={}, page_content='MEAP Edition \\nManning Early Access Program \\nFlink in Action \\nVersion 2 \\n \\n \\n \\n \\n \\nCopyright 2016 Manning Publications \\n \\n \\nFor more information on this and other Manning titles go to  \\nwww.manning.com \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action'), Document(metadata={}, page_content='1  \\nIntroducing Apache Flink \\nThis chapter covers  \\n• Why stream processing is important \\n• What is Apache Flink \\n• Apache Flink in the context of a real world example \\nThis book is about handling streaming data with Apache Flink. Every business is composed of \\na series of events. Imagine a large retail store or public news site that is serving customers all \\nover the world; events are constantly being generated. What distinguishes a streaming system \\nfrom a batch system is that the event stream is unbounded or infinite fro m a system \\nperspective. Decision -makers need to analyze these streaming events together to make \\nbusiness decisions. For example: \\n• A retail store chain is constantly selling products in various locations. People making \\ndecisions need to know how the various products are selling. Most current systems do \\nthis via nightly extract, transform, and load (ETL) processing, which is common in \\nenterprise environments, requires decision makers to wait an entire day before reports \\nbecome available. Ideally these decision  makers would like to be able to inquire in near \\nreal-time the performance of sales across the stores and regions.  \\n• A popular news website is constantly serving user requests. Each request/response can \\nbe considered an event. The stream of events need to be analyzed in near real -time to \\nunderstand how the news articles are performing with respect to page -views and to \\ndetermine which advertisements should be displayed to the readers as they are \\nbrowsing the website. \\n• Near real-time systems are especially valuable in fraud detection systems. Determining \\nthat a credit card transaction was a fraud within moments of performing it is crucial in \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action\\n1')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c35dcd9",
      "metadata": {
        "id": "3c35dcd9"
      },
      "source": [
        "## 8. Query those docs to get your answer back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f051337b",
      "metadata": {
        "id": "f051337b"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9b1c03",
      "metadata": {
        "id": "6b9b1c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0235cd-25c6-4bbe-ce7d-5261c975df63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2942243526.py:2: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67ea7c2",
      "metadata": {
        "id": "f67ea7c2"
      },
      "outputs": [],
      "source": [
        "query = \"What is Flink?\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfd2b7d",
      "metadata": {
        "id": "3dfd2b7d",
        "outputId": "5c3213d5-6c8f-4422-b259-9b3c68a06f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2708201610.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(input_documents=docs, question=query)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Flink is a stream processing system used for handling streaming data in real-time. It is commonly used in businesses to analyze and make decisions based on constantly generated events.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.(Optional) Using Pinecone as remote Vector Database\n",
        "Install minimal deps (Pinecone v2 client + LangChain split pkgs)"
      ],
      "metadata": {
        "id": "RqRfBXnXJ0lg"
      },
      "id": "RqRfBXnXJ0lg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -qU \"pinecone-client==3\" langchain-community langchain-openai tiktoken python-dotenv"
      ],
      "metadata": {
        "id": "9eCs0PGKJ8x7"
      },
      "id": "9eCs0PGKJ8x7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init Pinecone (v3) — connect & (optionally) create serverless index"
      ],
      "metadata": {
        "id": "65I1twsGJ_VQ"
      },
      "id": "65I1twsGJ_VQ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Init Pinecone (v3) — connect & ensure index\")\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# --- Config from your metadata ---\n",
        "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "assert pc_api_key, \"Missing PINECONE_API_KEY in environment.\"\n",
        "\n",
        "index_name_v3 = \"llama-2-rag\"    # from your metadata\n",
        "metric_v3 = \"cosine\"             # from your metadata\n",
        "dimension_v3 = 1536              # from your metadata\n",
        "cloud_v3 = \"aws\"                 # from your metadata\n",
        "region_v3 = \"us-east-1\"          # from your metadata\n",
        "\n",
        "# Init client\n",
        "pc = Pinecone(api_key=pc_api_key)\n",
        "\n",
        "# Create if not exists (serverless)\n",
        "existing_names = [it[\"name\"] for it in pc.list_indexes()]\n",
        "if index_name_v3 not in existing_names:\n",
        "    print(f\"Creating serverless index '{index_name_v3}' (dim={dimension_v3}, metric={metric_v3}, {cloud_v3}/{region_v3})\")\n",
        "    pc.create_index(\n",
        "        name=index_name_v3,\n",
        "        dimension=dimension_v3,\n",
        "        metric=metric_v3,\n",
        "        spec=ServerlessSpec(cloud=cloud_v3, region=region_v3),\n",
        "    )\n",
        "else:\n",
        "    print(f\"Index '{index_name_v3}' already exists.\")\n",
        "\n",
        "# Get an index handle (using name is fine; host can be used too)\n",
        "index_v3 = pc.Index(index_name_v3)\n",
        "\n",
        "# Show resolved host/summary\n",
        "info = pc.describe_index(index_name_v3)\n",
        "print(\"Connected to:\", info.get(\"name\"), \"| host:\", info.get(\"host\"), \"| metric:\", info.get(\"metric\"), \"| dim:\", info.get(\"dimension\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8m18QtNOWH",
        "outputId": "09380ae7-4577-4554-a1a3-bcede567eff7"
      },
      "id": "VD8m18QtNOWH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Init Pinecone (v3) — connect & ensure index\n",
            "Index 'llama-2-rag' already exists.\n",
            "Connected to: llama-2-rag | host: llama-2-rag-ac71174.svc.aped-4627-b74a.pinecone.io | metric: cosine | dim: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upsert texts into Pinecone (v3) — using your corpus"
      ],
      "metadata": {
        "id": "C3V77LZmNcVB"
      },
      "id": "C3V77LZmNcVB"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Upsert — embed & write your existing corpus into Pinecone (v3)\")\n",
        "\n",
        "from typing import Iterable\n",
        "\n",
        "# Reuse your existing embeddings if present; otherwise create one that matches dim=1536\n",
        "if \"embeddings\" not in globals():\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    assert openai_key, \"Missing OPENAI_API_KEY; set it in environment or .env.\"\n",
        "    # text-embedding-3-small → 1536 dims (matches your index)\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key, model=\"text-embedding-3-small\")\n",
        "    print(\"Created OpenAIEmbeddings(model='text-embedding-3-small').\")\n",
        "\n",
        "# Ensure your corpus exists\n",
        "assert \"texts\" in globals(), \"Variable `texts` not found. Please define your corpus upstream.\"\n",
        "\n",
        "def to_str_list(items: Iterable) -> list:\n",
        "    out = []\n",
        "    for x in items:\n",
        "        out.append(x.page_content if hasattr(x, \"page_content\") else str(x))\n",
        "    return out\n",
        "\n",
        "payloads: list = to_str_list(texts)\n",
        "\n",
        "# Embed and upsert\n",
        "vectors_upsert = []\n",
        "# Use embed_documents for batch embedding\n",
        "doc_vectors: List[List[float]] = embeddings.embed_documents(payloads)\n",
        "for i, (txt, vec) in enumerate(zip(payloads, doc_vectors)):\n",
        "    vectors_upsert.append({\"id\": f\"pc-doc-{i}\", \"values\": vec, \"metadata\": {\"text\": txt}})\n",
        "\n",
        "resp = index_v3.upsert(vectors=vectors_upsert)\n",
        "print(\"Upserted:\", len(vectors_upsert), \"vectors.\", \"Result:\", getattr(resp, \"upserted_count\", \"ok\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_7S1AW1NewX",
        "outputId": "3f191f9e-d1c8-4841-dc38-625b26159f22"
      },
      "id": "j_7S1AW1NewX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Upsert — embed & write your existing corpus into Pinecone (v3)\n",
            "Upserted: 51 vectors. Result: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity search (v3 query) — same “What is Flink?” case"
      ],
      "metadata": {
        "id": "Pcb19Ub7N5Sy"
      },
      "id": "Pcb19Ub7N5Sy"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Similarity Search — top-k for: 'What is Flink?'\")\n",
        "\n",
        "query_text_v3 = \"What is Flink?\"\n",
        "# Single-vector query\n",
        "qvec = embeddings.embed_query(query_text_v3)\n",
        "\n",
        "res = index_v3.query(\n",
        "    vector=qvec,\n",
        "    top_k=4,\n",
        "    include_metadata=True,\n",
        ")\n",
        "\n",
        "matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
        "if not matches:\n",
        "    print(\"No matches found.\")\n",
        "else:\n",
        "    for rank, m in enumerate(matches, 1):\n",
        "        md = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {})\n",
        "        snippet = (md.get(\"text\", \"\") or \"\")[:200].replace(\"\\n\", \" \")\n",
        "        score = m.get(\"score\", None) if isinstance(m, dict) else getattr(m, \"score\", None)\n",
        "        print(f\"[{rank}/4] id={m.get('id') if isinstance(m, dict) else getattr(m, 'id', None)} score={score} :: {snippet}{'...' if len(snippet)>=200 else ''}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_eYCDfTN8HX",
        "outputId": "91e53180-a84f-4ffa-d93b-dca310250932"
      },
      "id": "3_eYCDfTN8HX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Similarity Search — top-k for: 'What is Flink?'\n",
            "[1/4] id=pc-doc-4 score=0.839233398 :: 1   Introducing Apache Flink  This chapter covers   • Why stream processing is important  • What is Apache Flink  • Apache Flink in the context of a real world example  This book is about handling str...\n",
            "[2/4] id=pc-doc-9 score=0.835510254 :: https://forums.manning.com/forums/flink-in-action 4\n",
            "[3/4] id=pc-doc-50 score=0.834777832 :: streaming provides Flink with a more fine -grained ability to process data.  In the next chapter we will show you how to install Flink and write simple programs in Flink  using the DataSet, DataStream...\n",
            "[4/4] id=pc-doc-3 score=0.831787109 :: brief contents  PART 1:  STREAM PROCESSING USING FLINK    1  Introducing Apache Flink    2  Getting started with Flink     3  Batch processing using the DataSet API    4  Stream processing using the D...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever-style helper (v3) — returns LangChain Documents for QA"
      ],
      "metadata": {
        "id": "LEPysNPSN-gv"
      },
      "id": "LEPysNPSN-gv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Retriever Helper — convert v3 results to LangChain Documents\")\n",
        "\n",
        "from langchain.schema import Document\n",
        "\n",
        "def pinecone_v3_retrieve(question: str, k: int = 4) -> list[Document]:\n",
        "    qv = embeddings.embed_query(question)\n",
        "    out = index_v3.query(vector=qv, top_k=k, include_metadata=True)\n",
        "    m = out.get(\"matches\", []) if isinstance(out, dict) else getattr(out, \"matches\", [])\n",
        "    docs = []\n",
        "    for mm in m:\n",
        "        meta = mm.get(\"metadata\", {}) if isinstance(mm, dict) else getattr(mm, \"metadata\", {})\n",
        "        txt = meta.get(\"text\", \"\")\n",
        "        docs.append(Document(page_content=txt, metadata={k: v for k, v in meta.items() if k != \"text\"}))\n",
        "    return docs\n",
        "\n",
        "# Preview\n",
        "_preview = pinecone_v3_retrieve(\"What is Flink?\", k=3)\n",
        "for i, d in enumerate(_preview, 1):\n",
        "    print(f\"[doc {i}] {d.page_content[:120].replace('\\\\n',' ')}{'...' if len(d.page_content)>120 else ''}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH6Nuvv7OA0Q",
        "outputId": "6b8f53a4-7a24-4719-aa8a-d5db6f9dd1b1"
      },
      "id": "VH6Nuvv7OA0Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Retriever Helper — convert v3 results to LangChain Documents\n",
            "[doc 1] 1  \n",
            "Introducing Apache Flink \n",
            "This chapter covers  \n",
            "• Why stream processing is important \n",
            "• What is Apache Flink \n",
            "• Apac...\n",
            "[doc 2] https://forums.manning.com/forums/flink-in-action\n",
            "4\n",
            "[doc 3] streaming provides Flink with a more fine -grained ability to process data. \n",
            "In the next chapter we will show you how to...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) QA chain — same “What is Flink?” final answer"
      ],
      "metadata": {
        "id": "OS-gANK8OK2P"
      },
      "id": "OS-gANK8OK2P"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### QA — run the same final example with LangChain QA chain\")\n",
        "\n",
        "from langchain_openai import OpenAI as OpenAICompletion\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "qa_llm = OpenAICompletion(temperature=0)  # uses OPENAI_API_KEY from env\n",
        "qa_chain_v3 = load_qa_chain(qa_llm, chain_type=\"stuff\")\n",
        "\n",
        "question_v3 = \"What is Flink?\"\n",
        "docs_v3 = pinecone_v3_retrieve(question_v3, k=4)\n",
        "if not docs_v3:\n",
        "    print(\"No documents retrieved for QA.\")\n",
        "else:\n",
        "    answer_v3 = qa_chain_v3.run(input_documents=docs_v3, question=question_v3)\n",
        "    print(\"\\n### Answer\")\n",
        "    print(answer_v3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocWBi-UOQsf",
        "outputId": "83458bc5-cd4c-40b9-eb22-4a3af2f0406f"
      },
      "id": "7ocWBi-UOQsf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### QA — run the same final example with LangChain QA chain\n",
            "\n",
            "### Answer\n",
            " Flink is a stream processing system used for handling streaming data in real-time. It is commonly used in businesses to analyze and make decisions based on constantly generated events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Danger/Optional) Clear namespace or selective delete (v3)"
      ],
      "metadata": {
        "id": "T3YOIukaOZxX"
      },
      "id": "T3YOIukaOZxX"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Clear — delete all vectors in the default namespace (no explicit namespace)\")\n",
        "\n",
        "def pinecone_v3_clear_default():\n",
        "    # On this API version, do NOT pass namespace=\"__default__\".\n",
        "    index_v3.delete(delete_all=True)   # ← no namespace arg\n",
        "    print(f\"Cleared all vectors in the default namespace on index '{index_name_v3}'.\")\n",
        "\n",
        "# Uncomment to run:\n",
        "pinecone_v3_clear_default()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjnPy88kOcqv",
        "outputId": "645199a3-7f6e-482c-94fc-db40d8e8f613"
      },
      "id": "mjnPy88kOcqv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Clear — delete all vectors in the default namespace (no explicit namespace)\n",
            "Cleared all vectors in the default namespace on index 'llama-2-rag'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Relink an existing index handle (v3) — using host or name"
      ],
      "metadata": {
        "id": "Wj6bQSySOm7v"
      },
      "id": "Wj6bQSySOm7v"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Relink — show how to get a fresh handle to the same serverless index\")\n",
        "\n",
        "# Option 1: by name (simple for serverless)\n",
        "index_again = pc.Index(index_name_v3)\n",
        "\n",
        "# Option 2: by host (useful if you prefer explicit host)\n",
        "desc = pc.describe_index(index_name_v3)\n",
        "host_url = desc.get(\"host\")\n",
        "index_by_host = pc.Index(host=host_url)\n",
        "\n",
        "print(\"Relinked: name-handle ok, host-handle ok →\", host_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16IUj8e3OpVJ",
        "outputId": "0fcdba98-8ae6-41fa-b054-38ee98c90e53"
      },
      "id": "16IUj8e3OpVJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Relink — show how to get a fresh handle to the same serverless index\n",
            "Relinked: name-handle ok, host-handle ok → llama-2-rag-ac71174.svc.aped-4627-b74a.pinecone.io\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}