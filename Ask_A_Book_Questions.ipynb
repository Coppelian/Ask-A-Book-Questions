{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to ask a book a question?\n",
        "In this code demonstration, we're going to guide you through the following targets to understand how Retrieval Augmented Generation(RAG) works:\n",
        "\n",
        "\n",
        "1.   Loading Text information from PDFs/Online Sources.\n",
        "2.   Split texts and embed them into vectors.\n",
        "3.   Establish and store them in ChromaDB\n",
        "3.   Do similarity search and push the answers to ChatGPT, then get the answers.\n",
        "3.   **(Optional)** Use Pinecone as a remote Vector Database.\n",
        "\n",
        "![Fig.1 How does RAG works.](https://drive.google.com/uc?export=view&id=11xecWxSs5qiUQQHeY3lpkST0YLaOrjSw)\n",
        "\n",
        "Fig.1 How does RAG works.\n"
        "Ignore conflict errors if running on Colab.\n"
      ],
      "metadata": {
        "id": "h6rY7upEQ2gg"
      },
      "id": "h6rY7upEQ2gg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install/Upgrade Dependencies (Modern Stack)\n",
        "\n",
        "- Installs/updates packages\n",
        "- Removes RAPIDS (optional) in Google Colab to avoid pyarrow pinning\n",
        "- Installs the core stack (OpenAI/LangChain/Chroma/Pinecone etc.)\n",
        "\n",
        "Refs:\n",
        "- pip user guide: https://pip.pypa.io/\n",
        "- LangChain split pkgs: https://python.langchain.com/"
      ],
      "metadata": {
        "id": "atIWGj-kOeZ-"
      },
      "id": "atIWGj-kOeZ-"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d615a77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d615a77",
        "outputId": "de25f2e5-511f-40f4-e45a-960680fa7196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cudf-cu12 25.6.0\n",
            "Uninstalling cudf-cu12-25.6.0:\n",
            "  Successfully uninstalled cudf-cu12-25.6.0\n",
            "Found existing installation: pylibcudf-cu12 25.6.0\n",
            "Uninstalling pylibcudf-cu12-25.6.0:\n",
            "  Successfully uninstalled pylibcudf-cu12-25.6.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.6.0 requires cudf-cu12==25.6.*, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Update manual construction tool (latest-oriented setup)\n",
        "# 1) Optional: remove RAPIDS if present to avoid pyarrow constraints\n",
        "!pip uninstall -y cudf-cu12 pylibcudf-cu12 || true\n",
        "\n",
        "# 2) Upgrade build tools\n",
        "!pip install -qU pip setuptools wheel\n",
        "\n",
        "# 3) Base pins for this environment (latest compatible)\n",
        "#    - requests: you asked for 2.32.5 (note: Google Colab sometimes pins 2.32.4)\n",
        "!pip install -qU \"requests==2.32.5\" \"jedi>=0.16\"\n",
        "\n",
        "# 4) Install core stack (no old pinecone-client)\n",
        "#    - Use pyarrow>=21 to satisfy modern datasets\n",
        "!pip install -qU \\\n",
        "  \"pyarrow>=21\" \\\n",
        "  \"transformers>=4.45,<5\" \\\n",
        "  \"tokenizers>=0.20.1,<0.21\" \\\n",
        "  \"sentence-transformers>=3,<4\" \\\n",
        "  \"langchain>=0.3.27,<0.4\" \\\n",
        "  \"langchain-community>=0.3.7\" \\\n",
        "  \"langchain-openai>=0.2.12\" \\\n",
        "  \"datasets>=4.1.1\" \\\n",
        "  \"accelerate>=0.33\" \\\n",
        "  einops \\\n",
        "  pypdf \\\n",
        "  pdfminer.six \\\n",
        "  pinecone \\\n",
        "  chromadb\n",
        "\n",
        "# (Optional GPU extras)\n",
        "# pip install -qU xformers bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. PDF loader options (unstructured/PyPDF) and text splitting\n",
        "Shows two strategies to load PDFs (Unstructured vs PyPDF) and a text splitter configuration.\n",
        "\n",
        "\n",
        "*   **Unstructured loader** can parse complex PDFs but sometimes needs optional system deps.\n",
        "\n",
        "*   **PyPDFLoader** is simpler/reliable for most text PDFs.\n",
        "\n",
        "*   A splitter (e.g., **CharacterTextSplitter**) controls chunk size/overlap for embeddings.\n",
        "\n",
        "Please check Langchain Document for more informations: https://python.langchain.com/docs/integrations/document_loaders/\n",
        "\n"
      ],
      "metadata": {
        "id": "zoru1xJuSLF3"
      },
      "id": "zoru1xJuSLF3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2d3e92ed",
      "metadata": {
        "id": "2d3e92ed"
      },
      "outputs": [],
      "source": [
        "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b4a2d6bf",
      "metadata": {
        "id": "b4a2d6bf"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"Flink in Action.pdf\")\n",
        "# loader = PyPDFLoader(\"field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")\n",
        "## Other options for loaders\n",
        "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
        "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bcdac23c",
      "metadata": {
        "id": "bcdac23c"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b4fd7c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fd7c9e",
        "outputId": "f978e0c5-df71-400f-f9a9-3ed76dc194b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 35 document(s) in your data\n",
            "There are 2388 characters in your document\n"
          ]
        }
      ],
      "source": [
        "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[30].page_content)} characters in your document')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) How to load everything into data variable?"
      ],
      "metadata": {
        "id": "lUBw7AKyYeIu"
      },
      "id": "lUBw7AKyYeIu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use new document loader\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, OnlinePDFLoader, UnstructuredPDFLoader\n",
        ")\n",
        "\n",
        "loaders = [\n",
        "    PyPDFLoader(\"Flink in Action.pdf\"),\n",
        "    PyPDFLoader(\"field-guide-to-data-science.pdf\"),\n",
        "    OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\"),\n",
        "    UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\"),\n",
        "    # add more files/URL...\n",
        "]\n",
        "\n",
        "data = []\n",
        "for ld in loaders:\n",
        "    try:\n",
        "        docs = ld.load()          # every loader returns List[Document]\n",
        "        data.extend(docs)         # add to data\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] skip {ld.__class__.__name__}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(data)} Document chunks from {len(loaders)} loaders.\")\n",
        "# Optional: see the source\n",
        "sources = {d.metadata.get('source') for d in data}\n",
        "print(\"Unique sources:\", len(sources))"
      ],
      "metadata": {
        "id": "TBdTP-cOYkUF"
      },
      "id": "TBdTP-cOYkUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8af9b604",
      "metadata": {
        "id": "8af9b604"
      },
      "source": [
        "## 3. Chunk your data up into smaller documents\n",
        "* **How it works:** Builds/uses a TextSplitter to convert long page texts into smaller chunks for better embedding/search.\n",
        "\n",
        "* **Why:** Applies split_documents(docs) to get many shorter Document chunks optimized for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb3c6f02",
      "metadata": {
        "id": "fb3c6f02"
      },
      "outputs": [],
      "source": [
        "# Note: If you're using PyPDFLoader then we'll be splitting for the 2nd time.\n",
        "# This is optional, test out on your own data.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check an example\n",
        "print(texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8uqAi9AMnF7",
        "outputId": "b4e537c1-fb1d-41ff-d9d4-c1dc96841d36"
      },
      "id": "O8uqAi9AMnF7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='welcome \n",
            "Thank you for purchasing the MEAP for Flink in Action . We are excited to deliver this book as \n",
            "large-scale stream processing using Flink and Google Data Flow is fast gaining in popularity. \n",
            "Stream processing is much more than just processing records one at a time as they arrive. \n",
            "True stream processing needs support for concepts such as event time processing to ensure \n",
            "stream processing systems are just as accurate as the batch processing system. There is a \n",
            "need for one system the performs both stream a nd batch processing. Apache Flink is that \n",
            "system.  \n",
            "As we started exploring Apache Flink, we discovered the subtle challenges that are \n",
            "inherent in stream processing. These challenges are intrinsic to how stream processing is \n",
            "performed. Unlike batch processing, where all data is available when processing begins, \n",
            "stream processing must be able to handle incomplete data, late arrivals, and out -of-order \n",
            "arrivals—without compromising performance or accuracy —and be resilient to failure. We \n",
            "tackle all these challenges in this book.  \n",
            "Writing this book has been a challenge, partly because the technology is changing rapidly \n",
            "as we write and partly because we want to make this complex topic of streaming easy to \n",
            "understand in the context of everyday use cases. We believe that eventually streaming \n",
            "systems will become the norm, because the real world operates in the streaming mode. Real -\n",
            "world events occur and are captured continuously in transaction systems. The reporting \n",
            "systems that aggregate these transactions into reports operate in batch -processing mode due \n",
            "to technology limitations. These limitations are now being addressed by systems such as \n",
            "Apache Flink. We hope this book helps you develop a strong foundation in the concepts and \n",
            "the challenges of implementing streamin g systems capable of handling high -velocity and high-\n",
            "volume streaming data. \n",
            "Please be sure to post any questions, comments, or suggestions you have about the book' metadata={'producer': 'Adobe Acrobat Pro 10.0.0', 'creator': 'Adobe Acrobat Pro 10.0.0', 'creationdate': '2016-09-28T03:24:53+02:00', 'author': 'Sameer B. Wadkar and Hari Rajaram', 'moddate': '2016-09-28T03:29:37+02:00', 'subject': 'MEAP Version 2', 'title': 'Flink in Action MEAP V02', 'source': 'Flink in Action.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "879873a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879873a4",
        "outputId": "89612d2c-a094-47ac-a8a4-0066b9f321d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 51 documents\n"
          ]
        }
      ],
      "source": [
        "# Check the contents\n",
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b2843",
      "metadata": {
        "id": "838b2843"
      },
      "source": [
        "### 4. Create embeddings (OpenAI): Create embeddings of your documents to get ready for semantic search\n",
        "*   **Purpose:** Initializes OpenAIEmbeddings for vectorization\n",
        "\n",
        "*   **How does it works:** Uses your OPENAI_API_KEY and an embedding model (often text-embedding-3-small, 1536-d) to support similarity.\n",
        "\n",
        "*   **Notice:** Please make sure you have a .env file at the workspace containing OPENAI_API_KEY."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Environment Vars & Embeddings"
      ],
      "metadata": {
        "id": "A_G76aAPT2su"
      },
      "id": "A_G76aAPT2su"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "373e695a",
      "metadata": {
        "id": "373e695a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedccaf6-4a82-4485-dd4f-2581a04627d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma-only setup: embeddings ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 10\n",
        "import os\n",
        "\n",
        "# load OPENAI_API_KEY\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "assert OPENAI_API_KEY, \"Missing OPENAI_API_KEY. Put it in environment or a .env file.\"\n",
        "\n",
        "# Use OpenAI Embedding\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"Chroma-only setup: embeddings ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Init ChromaDB\n",
        "What we did is the following steps. Check Document to see how ChromaDB works: https://docs.trychroma.com/docs/overview/introduction\n",
        "\n",
        "\n",
        "1.   Establish local connections for ChromaDB\n",
        "2.   Convert text to vectors and insert them into local ChromaDB\n",
        "3.   Set up retriever.\n",
        "\n"
      ],
      "metadata": {
        "id": "sHneTBh_UBgF"
      },
      "id": "sHneTBh_UBgF"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0e093ef3",
      "metadata": {
        "hide_input": false,
        "id": "0e093ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d0c9ed-c64d-44e7-dd94-df2100dc8b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-277809409.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  chroma = Chroma(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma collection ready: llama-2-rag (persist at ./chroma_store)\n"
          ]
        }
      ],
      "source": [
        "CHROMA_DIR = \"./chroma_store\"     # Local Dir\n",
        "collection_name = \"llama-2-rag\"   # Name for the instance\n",
        "\n",
        "# load database\n",
        "chroma = Chroma(\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=CHROMA_DIR,\n",
        ")\n",
        "\n",
        "print(f\"Chroma collection ready: {collection_name} (persist at {CHROMA_DIR})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) How to create a ChromaDB and interact with it."
      ],
      "metadata": {
        "id": "I6mBgdXrXGvG"
      },
      "id": "I6mBgdXrXGvG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert datas into Chroma using Embedding\n",
        "from typing import Iterable\n",
        "\n",
        "def _to_strings(items: Iterable):\n",
        "    out = []\n",
        "    for t in items:\n",
        "        if hasattr(t, \"page_content\"):\n",
        "            out.append(t.page_content)\n",
        "        else:\n",
        "            out.append(str(t))\n",
        "    return out\n",
        "\n",
        "# Suppose we have `texts`, which we have defined above\n",
        "assert \"texts\" in globals(), \"Variable `texts` not found. Please define it before running this cell.\"\n",
        "\n",
        "text_list = _to_strings(texts)\n",
        "# If you need to manually define ids / metadatas，you can pass them in this API\n",
        "chroma.add_texts(texts=text_list)      # metadatas=..., ids=...\n",
        "chroma.persist()\n",
        "\n",
        "print(f\"Upserted {len(text_list)} texts into Chroma and persisted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5c_eaTX3KOL",
        "outputId": "b0407146-109b-46be-ff80-9db078f28008"
      },
      "id": "L5c_eaTX3KOL",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 51 texts into Chroma and persisted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4e0d1c6a",
      "metadata": {
        "id": "4e0d1c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ac689d-76cf-4b4a-9514-46f18be54a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever ready (Chroma).\n"
          ]
        }
      ],
      "source": [
        "retriever = chroma.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"Retriever ready (Chroma).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. (Danger/Optional) Clear Data Helper\n",
        "\n",
        "Clears all vectors in the given namespace (default: `__default__`)."
      ],
      "metadata": {
        "id": "LzPGLu5-SAj2"
      },
      "id": "LzPGLu5-SAj2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15\n",
        "def chroma_delete_by_ids(id_list):\n",
        "    \"\"\"Delete using ID\"\"\"\n",
        "    chroma.delete(ids=id_list)\n",
        "    chroma.persist()\n",
        "    print(f\"Deleted {len(id_list)} ids from Chroma.\")\n",
        "\n",
        "def chroma_delete_where(where: dict):\n",
        "    \"\"\"Delete Using Metadata\"\"\"\n",
        "    _ = chroma._collection.delete(where=where)\n",
        "    chroma.persist()\n",
        "    print(f\"Deleted by condition: {where}\")\n",
        "\n",
        "def chroma_clear_all():\n",
        "    \"\"\"Delete all\"\"\"\n",
        "    _ = chroma._collection.delete(where={})\n",
        "    chroma.persist()\n",
        "    print(f\"Cleared all data in Chroma collection '{collection_name}'.\")\n",
        "\n",
        "\n",
        "# chroma_delete_by_ids([\"doc-1\", \"doc-2\"])\n",
        "# chroma_delete_where({\"source\": {\"$eq\": \"note\"}})\n",
        "# chroma_clear_all()\n"
      ],
      "metadata": {
        "id": "vcA4F5BBR9z-"
      },
      "id": "vcA4F5BBR9z-",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Interact with ChromaDB"
      ],
      "metadata": {
        "id": "_fEz2WVnX7Td"
      },
      "id": "_fEz2WVnX7Td"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "388988ce",
      "metadata": {
        "id": "388988ce"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Use ChromaDB to construct Vector Database（Locally，Can change collection name and directory）\n",
        "docsearch = Chroma.from_texts(\n",
        "    texts=[t.page_content for t in texts],\n",
        "    embedding=embeddings,\n",
        "    collection_name=collection_name,\n",
        "    persist_directory=\"./chroma_store\",  # 本地存储路径\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "34929595",
      "metadata": {
        "id": "34929595"
      },
      "outputs": [],
      "source": [
        "query = \"What is flink?\"\n",
        "docs = docsearch.similarity_search(query, k=5)\n",
        "# Search similarity based on cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4e0f5b45",
      "metadata": {
        "id": "4e0f5b45",
        "outputId": "31c1acb8-f9a1-419e-8720-fe85b8c7168a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEAP Edition \n",
            "Manning Early Access Program \n",
            "Flink in Action \n",
            "Version 2 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Copyright 2016 Manning Publications \n",
            " \n",
            " \n",
            "For more information on this and other Manning titles go to  \n",
            "www.manning.com \n",
            "©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \n",
            "other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \n",
            "https://forums.manning.com/forums/flink-in-action\n"
          ]
        }
      ],
      "source": [
        "# Here's an example of the first document that was returned\n",
        "print(docs[3].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQb2UCn8M-p4",
        "outputId": "fd55c548-c3e6-4ba9-ae1f-f7ade0a429b0"
      },
      "id": "SQb2UCn8M-p4",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content='https://forums.manning.com/forums/flink-in-action\\n4'), Document(metadata={}, page_content='brief contents \\nPART 1:  STREAM PROCESSING USING FLINK \\n  1  Introducing Apache Flink \\n  2  Getting started with Flink  \\n  3  Batch processing using the DataSet API \\n  4  Stream processing using the DataStream API \\n  5  Basics of event time processing  \\nPART 2:  ADVANCED STREAM PROCESSING USING FLINK \\n  6  Session windows and custom windows \\n  7  Using the Flink API in practice  \\n  8  Using Kafka with Flink \\n  9  Fault tolerance in Flink   \\nPART 3:  OUT IN THE WILD  \\n10  Domain-specific libraries in Flink – CEP and Streaming SQL \\n11  Apache Beam and Flink \\nAPPENDIXES: \\nA   Setting up your local Flink environment \\nB   Installing Apache Kafka \\n \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action'), Document(metadata={}, page_content='streaming provides Flink with a more fine -grained ability to process data. \\nIn the next chapter we will show you how to install Flink and write simple programs in Flink \\nusing the DataSet, DataStream and Table APIs of Flink.  \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action\\n31'), Document(metadata={}, page_content='MEAP Edition \\nManning Early Access Program \\nFlink in Action \\nVersion 2 \\n \\n \\n \\n \\n \\nCopyright 2016 Manning Publications \\n \\n \\nFor more information on this and other Manning titles go to  \\nwww.manning.com \\n©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and \\nother simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders. \\nhttps://forums.manning.com/forums/flink-in-action')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c35dcd9",
      "metadata": {
        "id": "3c35dcd9"
      },
      "source": [
        "## 8. Query those docs to get your answer back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "f051337b",
      "metadata": {
        "id": "f051337b"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "6b9b1c03",
      "metadata": {
        "id": "6b9b1c03"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "f67ea7c2",
      "metadata": {
        "id": "f67ea7c2"
      },
      "outputs": [],
      "source": [
        "query = \"What is Flink?\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3dfd2b7d",
      "metadata": {
        "id": "3dfd2b7d",
        "outputId": "dddbe75e-556e-467a-da63-87ce1ef755a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Flink's architecture involves persisting events in a durable store (Kafka), reading them using a streaming source component, processing them in a Flink Pipeline composed of a graph of operators, and finally persisting the results in an external data store or data stream called a sink. The main steps in the stream processing are streaming events from upstream to downstream, with the first component being the streaming source and the last step being writing the data to the sink.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.(Optional) Using Pinecone as remote Vector Database\n",
        "Install minimal deps (Pinecone v2 client + LangChain split pkgs)"
      ],
      "metadata": {
        "id": "RqRfBXnXJ0lg"
      },
      "id": "RqRfBXnXJ0lg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -qU \"pinecone-client==3\" langchain-community langchain-openai tiktoken python-dotenv"
      ],
      "metadata": {
        "id": "9eCs0PGKJ8x7"
      },
      "id": "9eCs0PGKJ8x7",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init Pinecone (v3) — connect & (optionally) create serverless index"
      ],
      "metadata": {
        "id": "65I1twsGJ_VQ"
      },
      "id": "65I1twsGJ_VQ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Init Pinecone (v3) — connect & ensure index\")\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# --- Config from your metadata ---\n",
        "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "assert pc_api_key, \"Missing PINECONE_API_KEY in environment.\"\n",
        "\n",
        "index_name_v3 = \"llama-2-rag\"    # from your metadata\n",
        "metric_v3 = \"cosine\"             # from your metadata\n",
        "dimension_v3 = 1536              # from your metadata\n",
        "cloud_v3 = \"aws\"                 # from your metadata\n",
        "region_v3 = \"us-east-1\"          # from your metadata\n",
        "\n",
        "# Init client\n",
        "pc = Pinecone(api_key=pc_api_key)\n",
        "\n",
        "# Create if not exists (serverless)\n",
        "existing_names = [it[\"name\"] for it in pc.list_indexes()]\n",
        "if index_name_v3 not in existing_names:\n",
        "    print(f\"Creating serverless index '{index_name_v3}' (dim={dimension_v3}, metric={metric_v3}, {cloud_v3}/{region_v3})\")\n",
        "    pc.create_index(\n",
        "        name=index_name_v3,\n",
        "        dimension=dimension_v3,\n",
        "        metric=metric_v3,\n",
        "        spec=ServerlessSpec(cloud=cloud_v3, region=region_v3),\n",
        "    )\n",
        "else:\n",
        "    print(f\"Index '{index_name_v3}' already exists.\")\n",
        "\n",
        "# Get an index handle (using name is fine; host can be used too)\n",
        "index_v3 = pc.Index(index_name_v3)\n",
        "\n",
        "# Show resolved host/summary\n",
        "info = pc.describe_index(index_name_v3)\n",
        "print(\"Connected to:\", info.get(\"name\"), \"| host:\", info.get(\"host\"), \"| metric:\", info.get(\"metric\"), \"| dim:\", info.get(\"dimension\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8m18QtNOWH",
        "outputId": "f0623368-ffc0-458c-ce5d-8274fcdf2b39"
      },
      "id": "VD8m18QtNOWH",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Init Pinecone (v3) — connect & ensure index\n",
            "Index 'llama-2-rag' already exists.\n",
            "Connected to: llama-2-rag | host: llama-2-rag-ac71174.svc.aped-4627-b74a.pinecone.io | metric: cosine | dim: 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upsert texts into Pinecone (v3) — using your corpus"
      ],
      "metadata": {
        "id": "C3V77LZmNcVB"
      },
      "id": "C3V77LZmNcVB"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Upsert — embed & write your existing corpus into Pinecone (v3)\")\n",
        "\n",
        "from typing import Iterable\n",
        "\n",
        "# Reuse your existing embeddings if present; otherwise create one that matches dim=1536\n",
        "if \"embeddings\" not in globals():\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    assert openai_key, \"Missing OPENAI_API_KEY; set it in environment or .env.\"\n",
        "    # text-embedding-3-small → 1536 dims (matches your index)\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key, model=\"text-embedding-3-small\")\n",
        "    print(\"Created OpenAIEmbeddings(model='text-embedding-3-small').\")\n",
        "\n",
        "# Ensure your corpus exists\n",
        "assert \"texts\" in globals(), \"Variable `texts` not found. Please define your corpus upstream.\"\n",
        "\n",
        "def to_str_list(items: Iterable) -> list:\n",
        "    out = []\n",
        "    for x in items:\n",
        "        out.append(x.page_content if hasattr(x, \"page_content\") else str(x))\n",
        "    return out\n",
        "\n",
        "payloads: list = to_str_list(texts)\n",
        "\n",
        "# Embed and upsert\n",
        "vectors_upsert = []\n",
        "# Use embed_documents for batch embedding\n",
        "doc_vectors: List[List[float]] = embeddings.embed_documents(payloads)\n",
        "for i, (txt, vec) in enumerate(zip(payloads, doc_vectors)):\n",
        "    vectors_upsert.append({\"id\": f\"pc-doc-{i}\", \"values\": vec, \"metadata\": {\"text\": txt}})\n",
        "\n",
        "resp = index_v3.upsert(vectors=vectors_upsert)\n",
        "print(\"Upserted:\", len(vectors_upsert), \"vectors.\", \"Result:\", getattr(resp, \"upserted_count\", \"ok\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_7S1AW1NewX",
        "outputId": "0e608338-d18c-47d2-ebe7-bc7cbf3379c4"
      },
      "id": "j_7S1AW1NewX",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Upsert — embed & write your existing corpus into Pinecone (v3)\n",
            "Upserted: 51 vectors. Result: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity search (v3 query) — same “What is Flink?” case"
      ],
      "metadata": {
        "id": "Pcb19Ub7N5Sy"
      },
      "id": "Pcb19Ub7N5Sy"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Similarity Search — top-k for: 'What is Flink?'\")\n",
        "\n",
        "query_text_v3 = \"What is Flink?\"\n",
        "# Single-vector query\n",
        "qvec = embeddings.embed_query(query_text_v3)\n",
        "\n",
        "res = index_v3.query(\n",
        "    vector=qvec,\n",
        "    top_k=4,\n",
        "    include_metadata=True,\n",
        ")\n",
        "\n",
        "matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
        "if not matches:\n",
        "    print(\"No matches found.\")\n",
        "else:\n",
        "    for rank, m in enumerate(matches, 1):\n",
        "        md = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {})\n",
        "        snippet = (md.get(\"text\", \"\") or \"\")[:200].replace(\"\\n\", \" \")\n",
        "        score = m.get(\"score\", None) if isinstance(m, dict) else getattr(m, \"score\", None)\n",
        "        print(f\"[{rank}/4] id={m.get('id') if isinstance(m, dict) else getattr(m, 'id', None)} score={score} :: {snippet}{'...' if len(snippet)>=200 else ''}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_eYCDfTN8HX",
        "outputId": "9890ec0f-da9d-4bb2-946b-8f5b50efcc9b"
      },
      "id": "3_eYCDfTN8HX",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Similarity Search — top-k for: 'What is Flink?'\n",
            "No matches found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever-style helper (v3) — returns LangChain Documents for QA"
      ],
      "metadata": {
        "id": "LEPysNPSN-gv"
      },
      "id": "LEPysNPSN-gv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Retriever Helper — convert v3 results to LangChain Documents\")\n",
        "\n",
        "from langchain.schema import Document\n",
        "\n",
        "def pinecone_v3_retrieve(question: str, k: int = 4) -> list[Document]:\n",
        "    qv = embeddings.embed_query(question)\n",
        "    out = index_v3.query(vector=qv, top_k=k, include_metadata=True)\n",
        "    m = out.get(\"matches\", []) if isinstance(out, dict) else getattr(out, \"matches\", [])\n",
        "    docs = []\n",
        "    for mm in m:\n",
        "        meta = mm.get(\"metadata\", {}) if isinstance(mm, dict) else getattr(mm, \"metadata\", {})\n",
        "        txt = meta.get(\"text\", \"\")\n",
        "        docs.append(Document(page_content=txt, metadata={k: v for k, v in meta.items() if k != \"text\"}))\n",
        "    return docs\n",
        "\n",
        "# Preview\n",
        "_preview = pinecone_v3_retrieve(\"What is Flink?\", k=3)\n",
        "for i, d in enumerate(_preview, 1):\n",
        "    print(f\"[doc {i}] {d.page_content[:120].replace('\\\\n',' ')}{'...' if len(d.page_content)>120 else ''}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH6Nuvv7OA0Q",
        "outputId": "1ac58609-6466-476c-95eb-c36179481793"
      },
      "id": "VH6Nuvv7OA0Q",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Retriever Helper — convert v3 results to LangChain Documents\n",
            "[doc 1] 1  \n",
            "Introducing Apache Flink \n",
            "This chapter covers  \n",
            "• Why stream processing is important \n",
            "• What is Apache Flink \n",
            "• Apac...\n",
            "[doc 2] https://forums.manning.com/forums/flink-in-action\n",
            "4\n",
            "[doc 3] streaming provides Flink with a more fine -grained ability to process data. \n",
            "In the next chapter we will show you how to...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) QA chain — same “What is Flink?” final answer"
      ],
      "metadata": {
        "id": "OS-gANK8OK2P"
      },
      "id": "OS-gANK8OK2P"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### QA — run the same final example with LangChain QA chain\")\n",
        "\n",
        "from langchain_openai import OpenAI as OpenAICompletion\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "qa_llm = OpenAICompletion(temperature=0)  # uses OPENAI_API_KEY from env\n",
        "qa_chain_v3 = load_qa_chain(qa_llm, chain_type=\"stuff\")\n",
        "\n",
        "question_v3 = \"What is Flink?\"\n",
        "docs_v3 = pinecone_v3_retrieve(question_v3, k=4)\n",
        "if not docs_v3:\n",
        "    print(\"No documents retrieved for QA.\")\n",
        "else:\n",
        "    answer_v3 = qa_chain_v3.run(input_documents=docs_v3, question=question_v3)\n",
        "    print(\"\\n### Answer\")\n",
        "    print(answer_v3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocWBi-UOQsf",
        "outputId": "0325c4c5-b50a-4ae7-e30e-0d7e6a7c990c"
      },
      "id": "7ocWBi-UOQsf",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### QA — run the same final example with LangChain QA chain\n",
            "\n",
            "### Answer\n",
            " Flink is a stream processing system used for handling streaming data in real-time. It is commonly used in businesses to analyze and make decisions based on constantly generated events.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Danger/Optional) Clear namespace or selective delete (v3)"
      ],
      "metadata": {
        "id": "T3YOIukaOZxX"
      },
      "id": "T3YOIukaOZxX"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Clear — delete all vectors in the default namespace (no explicit namespace)\")\n",
        "\n",
        "def pinecone_v3_clear_default():\n",
        "    # On this API version, do NOT pass namespace=\"__default__\".\n",
        "    index_v3.delete(delete_all=True)   # ← no namespace arg\n",
        "    print(f\"Cleared all vectors in the default namespace on index '{index_name_v3}'.\")\n",
        "\n",
        "# Uncomment to run:\n",
        "pinecone_v3_clear_default()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjnPy88kOcqv",
        "outputId": "645199a3-7f6e-482c-94fc-db40d8e8f613"
      },
      "id": "mjnPy88kOcqv",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Clear — delete all vectors in the default namespace (no explicit namespace)\n",
            "Cleared all vectors in the default namespace on index 'llama-2-rag'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Relink an existing index handle (v3) — using host or name"
      ],
      "metadata": {
        "id": "Wj6bQSySOm7v"
      },
      "id": "Wj6bQSySOm7v"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Relink — show how to get a fresh handle to the same serverless index\")\n",
        "\n",
        "# Option 1: by name (simple for serverless)\n",
        "index_again = pc.Index(index_name_v3)\n",
        "\n",
        "# Option 2: by host (useful if you prefer explicit host)\n",
        "desc = pc.describe_index(index_name_v3)\n",
        "host_url = desc.get(\"host\")\n",
        "index_by_host = pc.Index(host=host_url)\n",
        "\n",
        "print(\"Relinked: name-handle ok, host-handle ok →\", host_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16IUj8e3OpVJ",
        "outputId": "0fcdba98-8ae6-41fa-b054-38ee98c90e53"
      },
      "id": "16IUj8e3OpVJ",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Relink — show how to get a fresh handle to the same serverless index\n",
            "Relinked: name-handle ok, host-handle ok → llama-2-rag-ac71174.svc.aped-4627-b74a.pinecone.io\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
